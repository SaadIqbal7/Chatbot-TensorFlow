{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Embedding, Lambda\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean sentence\n",
    "def clean_sent(text):\n",
    "    # Strip text\n",
    "    text = text.strip()\n",
    "    # Lower text\n",
    "    text = text.lower()\n",
    "    # Remove puntuations expect for space . ?\n",
    "    text = re.sub(r'[^\\w\\s\\?\\.]', '', text)\n",
    "    # Replace '?' with ' ?' and '.' with ' .'\n",
    "    text = re.sub(r'[\\?]', ' ?', text)\n",
    "    text = re.sub(r'[\\.]', ' .', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a few tokens which will be used in the vocabulary\n",
    "PAD = '<pad>'\n",
    "START = '<sos>'\n",
    "END = '<eos>'\n",
    "UNK = '<unk>'\n",
    "PAD_IDX = 0\n",
    "START_IDX = 1\n",
    "END_IDX = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the glove vector embeddings to sqlite\n",
    "# (Because the size of pretrained word embeddings is too large\n",
    "# we'll save the word embeddings in file and get embeddings from file)\n",
    "def load_from_glove_vector(path_glove):\n",
    "\tprint('Loading vocabulary...')\n",
    "\tword_index = {}\n",
    "\tindex_word = {}\n",
    "\t# Insert pad, start, end and unk tokens to vocabulary\n",
    "\tword_index[PAD] = PAD_IDX\n",
    "\tword_index[START] = START_IDX\n",
    "\tword_index[END] = END_IDX\n",
    "\tword_index[UNK] = UNK_IDX\n",
    "\tindex_word[PAD_IDX] = PAD\n",
    "\tindex_word[START_IDX] = START\n",
    "\tindex_word[END_IDX] = END\n",
    "\tindex_word[UNK_IDX] = UNK\n",
    "\n",
    "\tword_embedding = {}\n",
    "\tvocab_size = 4\n",
    "\tword_emb_sum = 0\n",
    "\twith open(path_glove, 'r', encoding=\"utf8\") as file:\n",
    "\t\t# Traverse each word -> word embedding pair\n",
    "\t\tlines = file.readlines()\n",
    "\t\tn = len(lines)\n",
    "\t\t# Change vocab size from here (if you want full vocabulary, just write range(0, n))\n",
    "\t\tfor i in range(0, 20000):\n",
    "\t\t\tline = lines[i]\n",
    "\t\t\t# Split word and word embeddings\n",
    "\t\t\tsplit_line = line.split(' ')\n",
    "\t\t\t# Get word\n",
    "\t\t\tword = split_line[0]\n",
    "\t\t\t# Get embeddings (in string)\n",
    "\t\t\tembedding = split_line[1:]\n",
    "\t\t\t# Save word -> word index\n",
    "\t\t\tword_index[word] = vocab_size\n",
    "\t\t\t# Save word index -> word\n",
    "\t\t\tindex_word[vocab_size] = word\n",
    "\t\t\t# Store index -> word embedding\n",
    "\t\t\tword_embedding[vocab_size] = np.array([float(value) for value in embedding])\n",
    "\t\t\t# Calculate sum of word embeddings\n",
    "\t\t\tword_emb_sum += word_embedding[vocab_size]\n",
    "\t\t\t# Increment vocabulary size\n",
    "\t\t\tvocab_size += 1\n",
    "\n",
    "\t\t# Get the length of one word embedding\n",
    "\t\temb_length = len(word_embedding[vocab_size - 1])\n",
    "\t\t\n",
    "\t\t# Add word embeddings for PAD START END UNK\n",
    "\t\t# Word embedding for PAD is all 0s (common for PAD token)\n",
    "\t\t# Word embedding for START is all 1s (as START token is at the start of the sequence)\n",
    "\t\t# Word embedding for END is all -1s (as END token is at the end of the sequence)\n",
    "\t\t# Word embedding for UNK is the average of all normal word embeddings (words other than tokens)\n",
    "\t\tword_embedding[PAD_IDX] = np.zeros((emb_length,))\n",
    "\t\tword_embedding[START_IDX] = np.ones((emb_length,))\n",
    "\t\tword_embedding[END_IDX] = np.zeros((emb_length,)) - 1\n",
    "\t\tword_embedding[UNK_IDX] = word_emb_sum / (vocab_size - 5)\n",
    "\n",
    "\t\tprint('Vocabulary loaded.')\n",
    "\n",
    "\t\treturn word_index,\\\n",
    "\t\t\tindex_word,\\\n",
    "\t\t\tdict(sorted(word_embedding.items(), key=lambda item: item[0])), \\\n",
    "\t\t\tvocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vocabulary using glove vectors\n",
    "word_index, index_word, word_embedding, vocab_size = load_from_glove_vector('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word to index table compatible with tensorflow (this will tokenize the sentences faster)\n",
    "word_index_table_init = tf.lookup.KeyValueTensorInitializer(\n",
    "    list(word_index.keys()), list(word_index.values()))\n",
    "\n",
    "word_index_table = tf.lookup.StaticHashTable(\n",
    "    word_index_table_init,\n",
    "    default_value=UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_example(example):\n",
    "    # Split example into input output pairs\n",
    "    io = tf.strings.split(example, sep=',')\n",
    "    # Tokenize example\n",
    "    io = tf.strings.split(io)\n",
    "    # Truncate sentences to 20 words\n",
    "    # Add START and END token in the output sentence\n",
    "    return io[0][:20], tf.concat([tf.concat([[START], io[1][:20]], axis=0), [END]], axis=0)\n",
    "\n",
    "dataset = tf.data.TextLineDataset('train_set.csv').map(split_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset example:')\n",
    "next(iter(dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp, out):\n",
    "    inp = word_index_table.lookup(inp)\n",
    "    out = word_index_table.lookup(out)\n",
    "    return inp, out\n",
    "\n",
    "dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset example after tokenization:')\n",
    "next(iter(dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify BATCH_SIZE\n",
    "BATCH_SIZE = 64\n",
    "dataset = dataset.shuffle(1000).padded_batch(BATCH_SIZE, padding_values=word_index[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation and test dataset\n",
    "# Limit validation and test values to 250 examples\n",
    "val_dataset = \\\n",
    "    tf.data\\\n",
    "    .TextLineDataset('val_set.csv')\\\n",
    "    .map(split_example)\\\n",
    "    .map(tokenize)\\\n",
    "    .padded_batch(250, padding_values=word_index[PAD])\n",
    "\n",
    "test_dataset = \\\n",
    "    tf.data\\\n",
    "    .TextLineDataset('test_set.csv')\\\n",
    "    .map(split_example)\\\n",
    "    .map(tokenize)\\\n",
    "    .padded_batch(250, padding_values=word_index[PAD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_units = 128\n",
    "dec_units = 128\n",
    "attention_units = 128\n",
    "\n",
    "# Get word embedding size\n",
    "word_embedding_size = len(word_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder class which implements LSTM to encode the input sentence\n",
    "\"\"\"\n",
    "class Encoder(Model):\n",
    "    def __init__(self, enc_units, embedding):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        # Set embedding layer\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize LSTM\n",
    "        self.lstm = LSTM(\n",
    "                enc_units,\n",
    "                return_sequences=True,\n",
    "                return_state=True,\n",
    "                recurrent_initializer='glorot_uniform'\n",
    "            )\n",
    "\n",
    "    def call(self, x, hidden_state, cell_state):\n",
    "        # x: (batch size, batch_max_input_length) np.ndarray\n",
    "        # hidden_state: initial hidden state (batch_size, enc_units)\n",
    "        # cell_state: initial cell state (batch_size, enc_units)\n",
    "        \n",
    "        # Get embeddings of word tokens\n",
    "        x = self.embedding(x) # (batch size, max_batch_input_length, embedding_size)\n",
    "\n",
    "        # Pass embeddings through lstm and get hiddens states (activations)\n",
    "        # (batch size, max_batch_input_length, enc_units)\n",
    "        activations, enc_hidden, enc_cell = \\\n",
    "            self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "\n",
    "        return activations, enc_hidden, enc_cell\n",
    "\n",
    "    def initialize_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units)), tf.zeros((batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attention class to compute bahdanau's attention\n",
    "\"\"\"\n",
    "class Attention(Model):\n",
    "    def __init__(self, attention_units):\n",
    "        super(Attention, self).__init__()\n",
    "        # Initialize dense layers to compute attention\n",
    "        self.W1 = Dense(attention_units)\n",
    "        self.W2 = Dense(attention_units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, activations, prev_hidden_state):\n",
    "        # activations: Activations of encoder (batch_size, batch_max_input_length, enc_units) tensor\n",
    "        # prev_hidden_state: Hidden state of previous decoder time step (batch_size, dec_units) tensor\n",
    "        # enc_pad_mask: Mask out the attention value for padded tokens (batch_size, batch_max_input_length)\n",
    "        \n",
    "        # Add extra dimension to hidden state\n",
    "        prev_hidden_state = tf.expand_dims(prev_hidden_state, axis=1) # (batch_size, 1, dec_units)\n",
    "        w1 = self.W1(prev_hidden_state) # (batch_size, 1, attention_units)\n",
    "        w2 = self.W2(activations) # (batch_size, batch_max_input_length, attention_units)\n",
    "        score = self.V(tf.nn.tanh(w1 + w2)) # (batch_size, batch_max_input_length, attention_units)\n",
    "\n",
    "        # Calculate attention from score\n",
    "        attention = tf.nn.softmax(score, axis=1) # (batch_size, batch_max_input_length, 1)        \n",
    "\n",
    "        # Calculate context vector\n",
    "        context = attention * activations # (batch_size, batch_max_input_length, enc_units)\n",
    "        context = tf.reduce_sum(context, axis=1) # (batch_size, enc_units)\n",
    "\n",
    "        return context, attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder class which implements LSTM to generate output sentence\n",
    "\"\"\"\n",
    "class Decoder(Model):\n",
    "    def __init__(self, dec_units, attention_units, embedding, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "\n",
    "        # Initialize attention layer\n",
    "        self.attention = Attention(attention_units)\n",
    "\n",
    "        # Set embedding layer\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize LSTM Unit\n",
    "        self.lstm = LSTM(\n",
    "            dec_units,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            return_state=True\n",
    "        )\n",
    "\n",
    "        # Initialize classification layer\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, activations, prev_hidden_state, prev_cell_state):\n",
    "        # x: Previous predicted token (batch_size, 1) np.ndarray\n",
    "        # activations: Activations of encoder (batch_size, batch_max_input_length, enc_units) tensor\n",
    "        # prev_hidden_state: hidden_state of previous cell (batch_size, dec_units) tensor\n",
    "        # prev_cell_state: cell_state of previous cell (batch_size, dec_units) tensor\n",
    "\n",
    "        # Calculate attention\n",
    "        # (batch_size, enc_units), (batch_size, match_max_input_length, 1)\n",
    "        context, attention = self.attention(activations, prev_hidden_state) \n",
    "\n",
    "        # Get word embedding of token\n",
    "        x = self.embedding(x) # (batch_size, 1, word_embedding_size)\n",
    "\n",
    "        # Concatenate word embedding with context vector\n",
    "        # (batch_size, 1, enc_units + word_embedding_size)\n",
    "        x = tf.concat([tf.expand_dims(context, axis=1), x], axis=-1)\n",
    "\n",
    "        # Pass the above vector through LSTM cell\n",
    "        # (batch_size, dec_units), (batch_size, dec_units) \n",
    "        _, hidden_state, cell_state = self.lstm(x, initial_state=[prev_hidden_state, prev_cell_state])\n",
    "\n",
    "        # Pass the hidden_state through classification layer\n",
    "        out = self.dense(hidden_state) # (batch_size, vocab_size)\n",
    "\n",
    "        return out, hidden_state, cell_state, attention\n",
    "\n",
    "    def initialize_state(self, BATCH_SIZE):\n",
    "        return tf.zeros((BATCH_SIZE, self.dec_units)), tf.zeros((BATCH_SIZE, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class that implements Seq2Seq model using LSTM encoder and decoder\n",
    "'''\n",
    "class Seq2Seq(Model):\n",
    "    def __init__(self, \n",
    "    enc_units, dec_units, attention_units, word_embedding, vocab_size, word_embedding_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # Initialize embedding layer\n",
    "        self.embedding = Embedding(\n",
    "            vocab_size, \n",
    "            word_embedding_size,\n",
    "            weights=[np.array(list(word_embedding.values()))],\n",
    "            trainable=False\n",
    "        )\n",
    "        # Initialize encoder\n",
    "        self.encoder = Encoder(enc_units, self.embedding)\n",
    "\n",
    "        # Initialize decoder\n",
    "        self.decoder = Decoder(dec_units, attention_units, self.embedding, vocab_size)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def call(self, inp, targ):\n",
    "        # inp: input batch (batch_size, batch_max_input_length)\n",
    "        # targ: ground truth (batch_size, batch_max_output_length)\n",
    "\n",
    "        # Initialize encoder states\n",
    "        enc_hidden, enc_cell = self.encoder.initialize_state(inp.shape[0])\n",
    "\n",
    "        # Get encoder output\n",
    "        activations, enc_hidden, enc_cell = self.encoder(inp, enc_hidden, enc_cell)\n",
    "\n",
    "        # Initialize decoder hidden and cell states\n",
    "        # ( Will give error if enc_units != dec_units. If you want that this line does not give any errors,\n",
    "        # either make dec_units = enc_units or remove this line and use -> \n",
    "        # dec_hidden, dec_cell = self.decoder.initialize_state(targ.shape[0]) )\n",
    "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        for t in range(0, targ.shape[1] - 1):\n",
    "            # Get decoder input\n",
    "            dec_input = tf.expand_dims(targ[:, t], axis=1)\n",
    "\n",
    "            # Get decoder output\n",
    "            pred, dec_hidden, dec_cell, _ = self.decoder(dec_input, activations, dec_hidden, dec_cell)\n",
    "\n",
    "            preds.append(pred) # (batch_max_input_length, batch_size, vocab_size)\n",
    "        \n",
    "        # (batch_size, batch_max_input_length, vocab_size)\n",
    "        return tf.reshape(tf.convert_to_tensor(preds), [inp.shape[0], -1, self.vocab_size])\n",
    "\n",
    "    def predict(self, inp):\n",
    "        # inp: input batch (batch_size, batch_max_input_length)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for sentence in inp: # (batch_max_input_length,)\n",
    "            # Initialize encoder states\n",
    "            enc_hidden, enc_cell = self.encoder.initialize_state(1) # (1, enc_units)\n",
    "            # Pass the sentence through encoder\n",
    "            # (1, batch_max_input_length, enc_units), (1, enc_units), (1, enc_units)\n",
    "            activations, enc_hidden, enc_cell = \\\n",
    "                self.encoder(tf.expand_dims(sentence, axis=0), enc_hidden, enc_cell)\n",
    "\n",
    "            # Initialize decoder states\n",
    "            dec_hidden, dec_cell = enc_hidden, enc_cell\n",
    "\n",
    "            max_sent_len = 22\n",
    "\n",
    "            dec_input = tf.expand_dims([word_index[START]], axis=0)\n",
    "            sent = [word_index[START]]\n",
    "            for _ in range(max_sent_len):\n",
    "                pred, dec_hidden, dec_cell, _ = self.decoder(dec_input, activations, dec_hidden, dec_cell)\n",
    "\n",
    "                # Get best word\n",
    "                index = tf.argmax(pred[0]).numpy()\n",
    "\n",
    "                sent.append(index)\n",
    "\n",
    "                # End output when EOS token is reached\n",
    "                if index == word_index[END]:\n",
    "                    break\n",
    "                \n",
    "                dec_input = tf.expand_dims([index], axis=0)\n",
    "            \n",
    "            outputs.append(sent)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(enc_units, dec_units, attention_units, word_embedding, vocab_size, word_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this line only if you have saved weights\n",
    "seq2seq.load_weights('model_weights/chatbot_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# Greater value of lambda -> greater regularization\n",
    "lamda = 0.001\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(gt, pred):\n",
    "    # gt: ground truth word: (batch_size, batch_max_output_length - 1)\n",
    "    # pred: predicted word: (batch_size, batch_max_output_length - 1, vocab_size)\n",
    "    \n",
    "    # Calculate mask\n",
    "    # Check if gt value == 0\n",
    "    # [3243, 0, 1234, 0, 0, 0] -> [False, True, False, True, True, True]\n",
    "    mask = tf.math.equal(gt, word_index[PAD])\n",
    "\n",
    "    # Reverse the value\n",
    "    # [False, True, False, True, True, True] -> [True, False, True, False, False, False]\n",
    "    mask = tf.math.logical_not(mask)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_object(gt, pred) # (batch_size,)\n",
    "\n",
    "    # Cast mask to same type as loss value's type (to floating point value more specifically)\n",
    "    # [True, False, True, False, False, False] -> [1, 0, 1, 0, 0, 0]\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "\n",
    "    # Multiply loss with mask\n",
    "    loss *= mask # (batch_size, batch_max_output_length - 1)\n",
    "\n",
    "    # Return mean of loss accross the batch\n",
    "    return tf.reduce_mean(loss) # No dims ()\n",
    "\n",
    "# Use this function in case your model is overfitting, change lamda to control how\n",
    "# much regularization you want\n",
    "def regularized_loss_function(gt, pred, weights):\n",
    "    # gt: ground truth word: (batch_size, batch_max_output_length - 1)\n",
    "    # pred: predicted word: (batch_size, batch_max_output_length - 1, vocab_size)\n",
    "    # weights: Weights of all layers (encoder + attention + decoder)\n",
    "\n",
    "    # Calculate simple loss across the batch\n",
    "    loss = loss_function(gt, pred) # No dims ()\n",
    "    \n",
    "    # Use l2 loss coefficient\n",
    "    l2_loss_coeff = (lamda / 2) * (tf.reduce_sum(weights ** 2)) # No dims ()\n",
    "    # Take the average of l2_loss_coeff across the batch\n",
    "    l2_loss_coeff = l2_loss_coeff / gt.shape[0]\n",
    "\n",
    "    # Add l2_loss_coeff to the original loss\n",
    "    loss += l2_loss_coeff\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(inp, targ):\n",
    "    # inp: input batch (batch_size, batch_max_input_length)\n",
    "    # targ: ground truth (batch_size, batch_max_output_length)\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    # Peform all the model calculations within the context of gradient tape so that \n",
    "    # gradient tape can calculate the gradient of loss wrt to model weights \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get predicts of model\n",
    "        preds = seq2seq(inp, targ)\n",
    "\n",
    "        # Calculate loss for each word in whole batch\n",
    "        batch_loss = loss_function(targ[:, 1:], preds)\n",
    "\n",
    "    # Get weights of model\n",
    "    weights = seq2seq.trainable_variables\n",
    "\n",
    "    # Calulate gradient of loss wrt to weights\n",
    "    gradients = tape.gradient(batch_loss, weights)\n",
    "\n",
    "    # Update weights of model using optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have saved accuracy and loss\n",
    "with open('model_weights/train_loss', mode='rb') as f:\n",
    "    train_loss = pickle.load(f)\n",
    "with open('model_weights/val_loss', mode='rb') as f:\n",
    "    val_loss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    TRAIN_LOSS = 0\n",
    "    VAL_LOSS = 0\n",
    "    \n",
    "    num_batches = 0\n",
    "\n",
    "    # -1 will get all batches, if you want to restrict the\n",
    "    # number of batches, just add a count. \n",
    "    # e.g. 1000 will give 1000 batches of size 64\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(1)):\n",
    "        batch_loss = train(inp, targ)\n",
    "\n",
    "        TRAIN_LOSS += batch_loss\n",
    "\n",
    "        if batch % 200 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "            print(template.format(\n",
    "                epoch + 1,\n",
    "                batch,\n",
    "                batch_loss.numpy()\n",
    "            ))\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    # Calculate validation loss\n",
    "    for val_inp, val_targ in val_dataset.take(1):\n",
    "        val_preds = seq2seq(val_inp, val_targ)\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    VAL_LOSS = loss_function(val_targ[:, 1:], val_preds)\n",
    "    \n",
    "    train_loss.append(TRAIN_LOSS.numpy()  / num_batches)\n",
    "    val_loss.append(VAL_LOSS.numpy())\n",
    "\n",
    "    # Save losses and model weights after each epoch    \n",
    "    seq2seq.save_weights('model_weights/chatbot_weights')\n",
    "\n",
    "    with open('model_weights/train_loss', mode='wb') as f:\n",
    "        pickle.dump(train_loss, f)\n",
    "    with open('model_weights/val_loss', mode='wb') as f:\n",
    "        pickle.dump(val_loss, f)\n",
    "\n",
    "    print('\\nEpoch {} Training Loss {:.4f}'\\\n",
    "        .format(epoch + 1, TRAIN_LOSS.numpy() / num_batches))\n",
    "\n",
    "    print('Epoch {} Validation Loss {:.4f}'\\\n",
    "        .format(epoch + 1, VAL_LOSS.numpy()))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss graph\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('LOSS')\n",
    "plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this line only if you have saved weights\n",
    "seq2seq.load_weights('model_weights/chatbot_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test set loss\n",
    "for test_inp, test_targ in test_dataset.take(1):\n",
    "    test_preds = seq2seq(test_inp, test_targ)\n",
    "\n",
    "# Calculate loss\n",
    "test_loss = loss_function(test_targ[:, 1:], test_preds)\n",
    "\n",
    "print('Test loss: ', test_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_output(sentence):\n",
    "    # Clean sentence\n",
    "    sentence = clean_sent(sentence)\n",
    "    # Tokenize sentence\n",
    "    sentence = sentence.split()\n",
    "    sentence = np.array([list(\n",
    "        map(lambda word: word_index[UNK] if word not in word_index else word_index[word], sentence))])\n",
    "\n",
    "    sentence = seq2seq.predict(sentence)[0]\n",
    "\n",
    "    sent = ''\n",
    "    for token in sentence:\n",
    "\n",
    "        if token != word_index[START] and token != word_index[END] and token != word_index[UNK]:\n",
    "            if token == word_index['?'] or token == word_index['.']:\n",
    "                sent += index_word[token]\n",
    "            else:\n",
    "                sent += ' ' + index_word[token]\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse():\n",
    "    while True:\n",
    "        # get input sentence from user\n",
    "        user_input = input('Enter sentence: ')\n",
    "        if user_input == 'end':\n",
    "            break\n",
    "\n",
    "        bot_output = get_bot_output(user_input)\n",
    "        print('Bot:', bot_output, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converse()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "0bf36e21-5cc3-4daa-9a41-197de28045e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}